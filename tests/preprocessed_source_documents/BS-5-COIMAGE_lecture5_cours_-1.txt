Page 1 1Image Processing Machine Learning Image Processing Page 2 machine learning 2 Arthur Samuel 1959 Machine learning field study gives computers ability learn without explicitly programmed Page 3 Machine learning everywhere 3Image Recognition Speech Recognition Stock Prediction Medical Diagnosis Data Analytics Robotics Page 4 4ML basics Page 5 Types machine learning 5Supervised Learning Training data known inputs outputs task example classification regression Page 6 Types machine learning 6Unsupervised Learning Training data inputs known output task example clustering try discover inherent properties data Page 7 Types machine learning 7Reinforcement Learning Program takes actions given environment maximize reward example video games Alpha Go robotics Page 8 Machine Learning Workflow 8Train Iterate till find best model Predict Integrate trained models applications Page 9 Loss functions 9How well algorithms model data low good prediction high bad prediction Monitored training process Example Mean squared error N number predictions yi reference value ŷi predicted value Page 10 Partitioning Data Sets 10 Divide two sets training set test set Recurrent mistake train test data Getting surprisingly low loss celebrating check youre accidentally training test data Page 11 Partitioning Data Sets 11Possible workflow Page 12 Partitioning Data Sets 12Generalization new data Page 13 Partitioning Data Sets 13 Better workflow use validation dataset Page 14 raw data features 14Goal Map part vector left one fields feature vector right Page 15 raw data features 15Mapping numerical values Page 16 raw data features 16Mapping categorical values Page 17 Feature Crosses 17A feature cross synthetic feature formed combining two features Crossing combinations features provide predictive abilities beyond features provide individually Data linearly separable Page 18 Feature Crosses 18We multiply two input features x B complex x B x C x x E B represent boolean features bins resulting crosses extremely sparse Page 19 Feature Crosses 19Why Linear learners use linear models scale well massive data without feature crosses expressivity models would limited Using feature crosses massive data one efficient strategy learning highly complex models Page 20 20ML algorithms Page 21 Classification 21In practice model outputs probability threshold value set get discrete classification Classification Algorithm normal pneumonia xlabel Supervised algorithm Goal Given observation x give class 1N Binary multiclass Page 22 Classification KNN 22KNearest Neighbor algorithm nonlinear function training required Algorithm 1choose K number neighbors 2for new sample compute distance every data sample 3take K nearest neighbors 4apply majority voting Page 23 Classification KNN 23Advantages Simple implementation training required Drawbacks Choice K prefer odd value Computation time big data set computing distance data points Sensitive noise Page 24 Regression 24Supervised algorithm Goal Given feature vector x predict corresponding continuous output value Example given number rooms house predict price house Page 25 Regression linear case 251D case linear function input feature x wx b ND case linear combination N input features x1 xN w1x1 w2x2 wNxN w0y 203 x 500 Page 26 Regression nonlinear case 26y nonlinear function input feature x Example polynomial regression fx w0 w1x w2x2 w3x3 wpxp w0 w1 w2 w3 wp 1 x x2 x3 xp wT Φx linear function apply linear regression new feature vector Page 27 Clustering 27Unsupervised ML algorithm groups data points clusters based similarity Example Kmeans algorithm Choice K Original image Kmeans gray levels Kmeans colors Kmeans algorithm image segmentation Page 28 28Evaluation ML algorithms Page 29 Evaluation metrics 29How evaluate models Depends task classification regression segmentation Page 30 Evaluation metrics classification 30Accuracy many items correctly predicted acc TP TN TP FP TN FN many cases accuracy poor misleading metric Typical case includes class imbalance positives negatives extremely rare Page 31 Evaluation metrics classification 31Precision model said positive class right precision TP TP FP Recall possible positives many model correctly identify recall TP TP FN Page 32 Evaluation metrics classification 32ROC curve receiver operating characteristic curve graph showing performance classification model classification thresholds Page 33 Evaluation metrics classification 33AUC Area ROC Curve Interpretation pick random positive random negative whats probability model ranks correct order example AUC 08 80 chance model able distinguish positive class negative class Intuition gives aggregate measure performance across possible classification thresholds better classification higher ROC higher value AUC Page 34 Evaluation metrics regression 34Mean Squared Error MSE 0 values correctly predicted Root mean squared error mean absolute error Page 35 35Example Cancerous cell detection Page 36 Example cancerous cell detection 36Task classification Approach Extract features raw images Train compare classifiers Test results new image data normal cancerous Machine Learning Data microscopy images marking DAPI Page 37 Example cancerous cell detection 37Step 1 preprocessing Improve image quality transformation filters Extract semantic content segmentation contours Correct content mathematical morphology Original image Normalize Otsu threshold Dilate Page 38 Example cancerous cell detection 38Step 2 Feature extraction extract cell Idea Connected Components connected component object binary image set adjacent pixels 4connectivity Two adjoining pixels part object connected along horizontal vertical direction 8connectivity Two adjoining pixels part object connected along horizontal vertical diagonal direction Page 39 Example cancerous cell detection 39Step 2 Feature extraction labelling connected components Idea Identify CC image assigning one unique label Page 40 Example cancerous cell detection 40Step 2 Feature extraction compute descriptors Shape measurements CC Area Perimeter Bounding Box Centroid Circularity 4Area πPerimeter² Eccentricity Extent Extremas Major minor axis length Orientation Page 41 Example cancerous cell detection 41Step 2 Feature extraction compute descriptors Pixel value measurements CC Max min intensity Mean intensity Pixel Values Weighted Centroid Page 42 Example cancerous cell detection 42Step 3 Choose train model Supervised Unsupervised Choose specific model Page 43 43Deep Learning Page 44 Deep Learning 44Perceptron artificial neuron Steps apply weight w input sum threshold get output Page 45 Deep Learning 45Perceptron training weights learned value modified optimize prediction minimize loss function Type model linear classifier Page 46 Deep Learning 46Multilayered perceptron MLP add hidden layers input output neurons model complex nonlinear functions Page 47 Deep Learning 47MLP image inputs Problem ignores spatial relationship pixels group neighbouring pixels correspond visual structures Page 48 Deep Learning 48Convolutional Neural Networks CNN use convolution layers pooling activation layers training process convolution kernels learned Page 49 Additional readings 49Andrew Ng Machine Learning Deep Learning course Ian Goodfellow deep learning book CNRS FIDLE course french